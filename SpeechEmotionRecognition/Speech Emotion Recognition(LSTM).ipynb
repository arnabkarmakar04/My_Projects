{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir = Path('C:\\\\Users\\\\arnab\\\\Desktop\\\\JUPYTER NOTEBOOK\\\\INTERNSHIP\\\\ZIDIO\\\\Speech_Data')\n",
    "audio_paths = list(dataset_dir.glob('**/*.wav'))\n",
    "emotion_labels = [os.path.split(os.path.split(path)[0])[1] for path in audio_paths]\n",
    "audio_df = pd.DataFrame({'audio_file': audio_paths, 'emotion': emotion_labels}).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_paths[:1])\n",
    "\n",
    "print(emotion_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Audio Processing Functions with Some Additional Tunings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    try:\n",
    "        data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "        return data, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):\n",
    "    noise_value = 0.015 * np.random.uniform() * np.amax(data)\n",
    "    return data + noise_value * np.random.normal(size=data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_audio(data, rate=0.9):\n",
    "    return librosa.effects.time_stretch(data, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feature Extraction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, sample_rate):\n",
    "    features = [\n",
    "        np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_fft=512).T, axis=0),\n",
    "    ]\n",
    "    return np.hstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_features(path):\n",
    "    data, sample_rate = load_audio(path)\n",
    "    if data is None:\n",
    "        return np.array([])\n",
    "    result = [extract_features(data, sample_rate)]\n",
    "    noisy_data = add_noise(data)\n",
    "    result.append(extract_features(noisy_data, sample_rate))\n",
    "    stretched_pitch = change_pitch(stretch_audio(data), sample_rate)\n",
    "    result.append(extract_features(stretched_pitch, sample_rate))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "for path, emotion in zip(audio_df.audio_file, audio_df.emotion):\n",
    "    features = export_features(path)\n",
    "    if features.size > 0:\n",
    "        for element in features:\n",
    "            X_train.append(element)\n",
    "            y_train.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataframe = pd.DataFrame(X_train)\n",
    "feature_dataframe['EMOTIONS'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_dataframe['EMOTIONS'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Checking Some Audio Samples With Above Tunings & Plotting Them***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, speech = read(audio_df['audio_file'][2342])\n",
    "print(audio_df['emotion'][2342])\n",
    "Audio(speech, rate=rate, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, speech = read(audio_df['audio_file'][20])\n",
    "print(audio_df['emotion'][20])\n",
    "Audio(speech, rate=rate, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "audio_speech,rate = librosa.load(audio_df['audio_file'][120])\n",
    "print(audio_df['emotion'][120])\n",
    "librosa.display.waveshow(audio_speech, sr=rate, color = 'orange')\n",
    "Audio(audio_speech, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "audio_speech,rate = librosa.load(audio_df['audio_file'][10])\n",
    "print(audio_df['emotion'][10])\n",
    "librosa.display.waveshow(audio_speech, sr=rate, color = 'green')\n",
    "Audio(audio_speech, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "plt.title(\"Tune 1: Voice With Some Noise\")\n",
    "audio_speech,sample_rate = librosa.load(audio_df['audio_file'][2000])\n",
    "print(audio_df['emotion'][2000])\n",
    "noise_injection = add_noise(audio_speech)\n",
    "librosa.display.waveshow(noise_injection, sr=sample_rate)\n",
    "Audio(noise_injection, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "plt.title(\"Tune 2: Streched Voice\")\n",
    "audio_speech,sample_rate = librosa.load(audio_df['audio_file'][2000])\n",
    "print(audio_df['emotion'][2000])\n",
    "stretching_audio = stretch_audio(audio_speech)\n",
    "librosa.display.waveshow(stretching_audio, sr=sample_rate, color='red')\n",
    "Audio(stretching_audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Spectrogram of a Audio Sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "audio_speech, rate = librosa.load(audio_df['audio_file'][2000])\n",
    "stft_audio = librosa.stft(audio_speech)\n",
    "Db_audio = librosa.amplitude_to_db(abs(stft_audio))\n",
    "librosa.display.specshow(Db_audio, sr=rate, x_axis='time', y_axis='hz')\n",
    "plt.title('Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visualization of MFCCs of a Audio Sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file, sr = librosa.load(audio_df['audio_file'][2000])\n",
    "mfccs = librosa.feature.mfcc(y=audio_file, sr=sr, n_mfcc=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap='cool')\n",
    "plt.title('Mel-Frequency Cepstral Coefficients (MFCCs)')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder_label = OneHotEncoder()\n",
    "\n",
    "X = feature_dataframe.iloc[:, :-1].values\n",
    "emotions_array = feature_dataframe['EMOTIONS'].values.reshape(-1, 1)\n",
    "Y = encoder_label.fit_transform(emotions_array).toarray()\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_data = StandardScaler()\n",
    "\n",
    "X_train = scaler_data.fit_transform(X_train)\n",
    "X_test = scaler_data.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Creation (LSTM)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1\n",
    "num_features = X_train.shape[1]\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_lstm_model((timesteps, num_features), Y.shape[1])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "callbacks = [ModelCheckpoint('speech-emotion-recognition.keras', verbose=1, save_best_only=True)]\n",
    "history = lstm_model.fit(X_train, y_train, batch_size=64, epochs=50, callbacks=callbacks, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visualization of Accuracy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='red', marker='o', linestyle='-')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue', marker='o', linestyle='-')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='red', marker='o', linestyle='-')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='blue', marker='o', linestyle='-')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test Output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('speech-emotion-recognition.keras')\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(X_test)\n",
    "y_pred = encoder_label.inverse_transform(test_prediction)\n",
    "y_test_inv = encoder_label.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_inv[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Confusion Matrix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_prediction, axis=1))\n",
    "plt.figure(figsize=(13, 6))\n",
    "sns.heatmap(conf_matrix, linecolor='white', cmap='Blues', annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classification Report***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_inv, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
